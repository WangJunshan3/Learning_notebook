{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Coursera Link](https://www.coursera.org/specializations/deep-learning#courses)\n",
    "# COURSE 2: Improving Deep Neural Networks - Hyperparameter tuning, Regularization and Optimization\n",
    "**About this course**  \n",
    "This course will teach you the \"magic\" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. After 3 weeks, you will: - Understand industry best-practices for building deep learning applications. - Be able to effectively use the common neural network \"tricks\", including initialization, L2 and dropout regularization, Batch normalization, gradient checking, - Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. - Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance - Be able to implement a neural network in TensorFlow. This is the second course of the Deep Learning Specialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#COURSE-2:-Improving-Deep-Neural-Networks---Hyperparameter-tuning,-Regularization-and-Optimization\" data-toc-modified-id=\"COURSE-2:-Improving-Deep-Neural-Networks---Hyperparameter-tuning,-Regularization-and-Optimization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>COURSE 2: Improving Deep Neural Networks - Hyperparameter tuning, Regularization and Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#WEEK-1:-Practical-aspects-of-Deep-Learning\" data-toc-modified-id=\"WEEK-1:-Practical-aspects-of-Deep-Learning-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>WEEK 1: Practical aspects of Deep Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#--Video-·-Train-/-Dev-/-Test-sets\" data-toc-modified-id=\"--Video-·-Train-/-Dev-/-Test-sets-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span><ins> <em> Video · Train / Dev / Test sets</em></ins></a></span></li><li><span><a href=\"#--Video-·-Bias-/-Variance\" data-toc-modified-id=\"--Video-·-Bias-/-Variance-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span><ins> <em> Video · Bias / Variance</em></ins></a></span></li><li><span><a href=\"#--Video-·-Basic-Recipe-for-Machine-Learning\" data-toc-modified-id=\"--Video-·-Basic-Recipe-for-Machine-Learning-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span><ins> <em> Video · Basic Recipe for Machine Learning</em></ins></a></span></li><li><span><a href=\"#--Video-·-Regularization\" data-toc-modified-id=\"--Video-·-Regularization-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span><ins> <em> Video · Regularization</em></ins></a></span></li><li><span><a href=\"#--Video-·-Why-regularization-reduces-overfitting?\" data-toc-modified-id=\"--Video-·-Why-regularization-reduces-overfitting?-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span><ins> <em> Video · Why regularization reduces overfitting?</em></ins></a></span></li><li><span><a href=\"#--Video-·-Dropout-Regularization\" data-toc-modified-id=\"--Video-·-Dropout-Regularization-2.1.6\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span><ins> <em> Video · Dropout Regularization</em></ins></a></span></li><li><span><a href=\"#--Video-·-Understanding-Dropout\" data-toc-modified-id=\"--Video-·-Understanding-Dropout-2.1.7\"><span class=\"toc-item-num\">2.1.7&nbsp;&nbsp;</span><ins> <em> Video · Understanding Dropout</em></ins></a></span></li><li><span><a href=\"#--Video-·-Other-regularization-methods\" data-toc-modified-id=\"--Video-·-Other-regularization-methods-2.1.8\"><span class=\"toc-item-num\">2.1.8&nbsp;&nbsp;</span><ins> <em> Video · Other regularization methods</em></ins></a></span></li><li><span><a href=\"#--Video-·-Normalizing-inputs\" data-toc-modified-id=\"--Video-·-Normalizing-inputs-2.1.9\"><span class=\"toc-item-num\">2.1.9&nbsp;&nbsp;</span><ins> <em> Video · Normalizing inputs</em></ins></a></span></li><li><span><a href=\"#--Video-·-Vanishing-/-Exploding-gradients\" data-toc-modified-id=\"--Video-·-Vanishing-/-Exploding-gradients-2.1.10\"><span class=\"toc-item-num\">2.1.10&nbsp;&nbsp;</span><ins> <em> Video · Vanishing / Exploding gradients</em></ins></a></span></li><li><span><a href=\"#--Video-·-Weight-Initialization-for-Deep-Networks\" data-toc-modified-id=\"--Video-·-Weight-Initialization-for-Deep-Networks-2.1.11\"><span class=\"toc-item-num\">2.1.11&nbsp;&nbsp;</span><ins> <em> Video · Weight Initialization for Deep Networks</em></ins></a></span></li><li><span><a href=\"#--Video-·-Numerical-approximation-of-gradients\" data-toc-modified-id=\"--Video-·-Numerical-approximation-of-gradients-2.1.12\"><span class=\"toc-item-num\">2.1.12&nbsp;&nbsp;</span><ins> <em> Video · Numerical approximation of gradients</em></ins></a></span></li><li><span><a href=\"#--Video-·-Gradient-checking\" data-toc-modified-id=\"--Video-·-Gradient-checking-2.1.13\"><span class=\"toc-item-num\">2.1.13&nbsp;&nbsp;</span><ins> <em> Video · Gradient checking</em></ins></a></span></li><li><span><a href=\"#--Video-·-Gradient-Checking-Implementation-Notes\" data-toc-modified-id=\"--Video-·-Gradient-Checking-Implementation-Notes-2.1.14\"><span class=\"toc-item-num\">2.1.14&nbsp;&nbsp;</span><ins> <em> Video · Gradient Checking Implementation Notes</em></ins></a></span></li><li><span><a href=\"#--Quiz-·-Practical-aspects-of-deep-learning\" data-toc-modified-id=\"--Quiz-·-Practical-aspects-of-deep-learning-2.1.15\"><span class=\"toc-item-num\">2.1.15&nbsp;&nbsp;</span><ins> <em> Quiz · Practical aspects of deep learning</em></ins></a></span></li><li><span><a href=\"#--Other-·-Initialization\" data-toc-modified-id=\"--Other-·-Initialization-2.1.16\"><span class=\"toc-item-num\">2.1.16&nbsp;&nbsp;</span><ins> <em> Other · Initialization</em></ins></a></span></li><li><span><a href=\"#--Programming-Assignment-·-Initialization\" data-toc-modified-id=\"--Programming-Assignment-·-Initialization-2.1.17\"><span class=\"toc-item-num\">2.1.17&nbsp;&nbsp;</span><ins> <em> Programming Assignment · Initialization</em></ins></a></span></li><li><span><a href=\"#--Other-·-Regularization\" data-toc-modified-id=\"--Other-·-Regularization-2.1.18\"><span class=\"toc-item-num\">2.1.18&nbsp;&nbsp;</span><ins> <em> Other · Regularization</em></ins></a></span></li><li><span><a href=\"#--Programming-Assignment-·-Regularization\" data-toc-modified-id=\"--Programming-Assignment-·-Regularization-2.1.19\"><span class=\"toc-item-num\">2.1.19&nbsp;&nbsp;</span><ins> <em> Programming Assignment · Regularization</em></ins></a></span></li><li><span><a href=\"#--Other-·-Gradient-Checking\" data-toc-modified-id=\"--Other-·-Gradient-Checking-2.1.20\"><span class=\"toc-item-num\">2.1.20&nbsp;&nbsp;</span><ins> <em> Other · Gradient Checking</em></ins></a></span></li><li><span><a href=\"#--Programming-Assignment-·-Gradient-Checking\" data-toc-modified-id=\"--Programming-Assignment-·-Gradient-Checking-2.1.21\"><span class=\"toc-item-num\">2.1.21&nbsp;&nbsp;</span><ins> <em> Programming Assignment · Gradient Checking</em></ins></a></span></li></ul></li><li><span><a href=\"#WEEK-2:-Optimization-algorithms\" data-toc-modified-id=\"WEEK-2:-Optimization-algorithms-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>WEEK 2: Optimization algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#--Video-·-Mini-batch-gradient-descent\" data-toc-modified-id=\"--Video-·-Mini-batch-gradient-descent-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span><ins> <em> Video · Mini-batch gradient descent</em></ins></a></span></li><li><span><a href=\"#--Video-·-Understanding-mini-batch-gradient-descent\" data-toc-modified-id=\"--Video-·-Understanding-mini-batch-gradient-descent-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span><ins> <em> Video · Understanding mini-batch gradient descent</em></ins></a></span></li><li><span><a href=\"#--Video-·-Exponentially-weighted-averages\" data-toc-modified-id=\"--Video-·-Exponentially-weighted-averages-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span><ins> <em> Video · Exponentially weighted averages</em></ins></a></span></li><li><span><a href=\"#--Video-·-Understanding-exponentially-weighted-averages\" data-toc-modified-id=\"--Video-·-Understanding-exponentially-weighted-averages-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span><ins> <em> Video · Understanding exponentially weighted averages</em></ins></a></span></li><li><span><a href=\"#--Video-·-Bias-correction-in-exponentially-weighted-averages\" data-toc-modified-id=\"--Video-·-Bias-correction-in-exponentially-weighted-averages-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span><ins> <em> Video · Bias correction in exponentially weighted averages</em></ins></a></span></li><li><span><a href=\"#--Video-·-Gradient-descent-with-momentum\" data-toc-modified-id=\"--Video-·-Gradient-descent-with-momentum-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span><ins> <em> Video · Gradient descent with momentum</em></ins></a></span></li><li><span><a href=\"#--Video-·-RMSprop\" data-toc-modified-id=\"--Video-·-RMSprop-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span><ins> <em> Video · RMSprop</em></ins></a></span></li><li><span><a href=\"#--Video-·-Adam-optimization-algorithm\" data-toc-modified-id=\"--Video-·-Adam-optimization-algorithm-2.2.8\"><span class=\"toc-item-num\">2.2.8&nbsp;&nbsp;</span><ins> <em> Video · Adam optimization algorithm</em></ins></a></span></li><li><span><a href=\"#--Video-·-Learning-rate-decay\" data-toc-modified-id=\"--Video-·-Learning-rate-decay-2.2.9\"><span class=\"toc-item-num\">2.2.9&nbsp;&nbsp;</span><ins> <em> Video · Learning rate decay</em></ins></a></span></li><li><span><a href=\"#--Video-·-The-problem-of-local-optima\" data-toc-modified-id=\"--Video-·-The-problem-of-local-optima-2.2.10\"><span class=\"toc-item-num\">2.2.10&nbsp;&nbsp;</span><ins> <em> Video · The problem of local optima</em></ins></a></span></li><li><span><a href=\"#--Quiz-·-Optimization-algorithms\" data-toc-modified-id=\"--Quiz-·-Optimization-algorithms-2.2.11\"><span class=\"toc-item-num\">2.2.11&nbsp;&nbsp;</span><ins> <em> Quiz · Optimization algorithms</em></ins></a></span></li><li><span><a href=\"#--Other-·-Optimization\" data-toc-modified-id=\"--Other-·-Optimization-2.2.12\"><span class=\"toc-item-num\">2.2.12&nbsp;&nbsp;</span><ins> <em> Other · Optimization</em></ins></a></span></li><li><span><a href=\"#--Programming-Assignment-·-Optimization\" data-toc-modified-id=\"--Programming-Assignment-·-Optimization-2.2.13\"><span class=\"toc-item-num\">2.2.13&nbsp;&nbsp;</span><ins> <em> Programming Assignment · Optimization</em></ins></a></span></li></ul></li><li><span><a href=\"#WEEK-3:-Hyperparameter-tuning,-Batch-Normalization-and-Programming-Frameworks\" data-toc-modified-id=\"WEEK-3:-Hyperparameter-tuning,-Batch-Normalization-and-Programming-Frameworks-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>WEEK 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks</a></span><ul class=\"toc-item\"><li><span><a href=\"#--Video-·-Tuning-process\" data-toc-modified-id=\"--Video-·-Tuning-process-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span><ins> <em> Video · Tuning process</em></ins></a></span></li><li><span><a href=\"#--Video-·-Using-an-appropriate-scale-to-pick-hyperparameters\" data-toc-modified-id=\"--Video-·-Using-an-appropriate-scale-to-pick-hyperparameters-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span><ins> <em> Video · Using an appropriate scale to pick hyperparameters</em></ins></a></span></li><li><span><a href=\"#--Video-·-Hyperparameters-tuning-in-practice:-Pandas-vs.-Caviar\" data-toc-modified-id=\"--Video-·-Hyperparameters-tuning-in-practice:-Pandas-vs.-Caviar-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span><ins> <em> Video · Hyperparameters tuning in practice: Pandas vs. Caviar</em></ins></a></span></li><li><span><a href=\"#--Video-·-Normalizing-activations-in-a-network\" data-toc-modified-id=\"--Video-·-Normalizing-activations-in-a-network-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span><ins> <em> Video · Normalizing activations in a network</em></ins></a></span></li><li><span><a href=\"#--Video-·-Fitting-Batch-Norm-into-a-neural-network\" data-toc-modified-id=\"--Video-·-Fitting-Batch-Norm-into-a-neural-network-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span><ins> <em> Video · Fitting Batch Norm into a neural network</em></ins></a></span></li><li><span><a href=\"#--Video-·-Why-does-Batch-Norm-work?\" data-toc-modified-id=\"--Video-·-Why-does-Batch-Norm-work?-2.3.6\"><span class=\"toc-item-num\">2.3.6&nbsp;&nbsp;</span><ins> <em> Video · Why does Batch Norm work?</em></ins></a></span></li><li><span><a href=\"#--Video-·-Batch-Norm-at-test-time\" data-toc-modified-id=\"--Video-·-Batch-Norm-at-test-time-2.3.7\"><span class=\"toc-item-num\">2.3.7&nbsp;&nbsp;</span><ins> <em> Video · Batch Norm at test time</em></ins></a></span></li><li><span><a href=\"#--Video-·-Softmax-Regression\" data-toc-modified-id=\"--Video-·-Softmax-Regression-2.3.8\"><span class=\"toc-item-num\">2.3.8&nbsp;&nbsp;</span><ins> <em> Video · Softmax Regression</em></ins></a></span></li><li><span><a href=\"#--Video-·-Training-a-softmax-classifier\" data-toc-modified-id=\"--Video-·-Training-a-softmax-classifier-2.3.9\"><span class=\"toc-item-num\">2.3.9&nbsp;&nbsp;</span><ins> <em> Video · Training a softmax classifier</em></ins></a></span></li><li><span><a href=\"#--Video-·-Deep-learning-frameworks\" data-toc-modified-id=\"--Video-·-Deep-learning-frameworks-2.3.10\"><span class=\"toc-item-num\">2.3.10&nbsp;&nbsp;</span><ins> <em> Video · Deep learning frameworks</em></ins></a></span></li><li><span><a href=\"#--Video-·-TensorFlow\" data-toc-modified-id=\"--Video-·-TensorFlow-2.3.11\"><span class=\"toc-item-num\">2.3.11&nbsp;&nbsp;</span><ins> <em> Video · TensorFlow</em></ins></a></span></li><li><span><a href=\"#--Quiz-·-Hyperparameter-tuning,-Batch-Normalization,-Programming-Frameworks\" data-toc-modified-id=\"--Quiz-·-Hyperparameter-tuning,-Batch-Normalization,-Programming-Frameworks-2.3.12\"><span class=\"toc-item-num\">2.3.12&nbsp;&nbsp;</span><ins> <em> Quiz · Hyperparameter tuning, Batch Normalization, Programming Frameworks</em></ins></a></span></li><li><span><a href=\"#--Other-·-Tensorflow\" data-toc-modified-id=\"--Other-·-Tensorflow-2.3.13\"><span class=\"toc-item-num\">2.3.13&nbsp;&nbsp;</span><ins> <em> Other · Tensorflow</em></ins></a></span></li><li><span><a href=\"#--Programming-Assignment-·-Tensorflow\" data-toc-modified-id=\"--Programming-Assignment-·-Tensorflow-2.3.14\"><span class=\"toc-item-num\">2.3.14&nbsp;&nbsp;</span><ins> <em> Programming Assignment · Tensorflow</em></ins></a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEEK 1: Practical aspects of Deep Learning\n",
    "Learn to build a neural network with one hidden layer, using forward propagation and backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Train / Dev / Test sets*</ins>\n",
    "###### <span style=\"color:red\">  My Comment </span>\n",
    "* ML application: different sceniaro has different suitable hyperparameter\n",
    "    * -> Iterative process: Idea -> Code -> Experiment -> Idea\n",
    "* Training set: initialization of hyperparameters\n",
    "* Development set / validation set: refine and adjust hyperparameters; test for different methods, just need enough size to evaluate the performance of methods.\n",
    "* Test set\n",
    "* Remarks:\n",
    "    * ratio: traditionaly be set as: 60/20/20, now very large data set: 98/1/1  or even 99.5/0.02/0.03 \n",
    "    * <span style=\"color:red\">  dev set and test set must come form the same distribution </span>\n",
    "    * OK to not have the test set (dev set only)\n",
    "        * Might overfitting of model to the dev (test) set in this case.\n",
    "        \n",
    "### <ins> * Video · Bias / Variance*</ins>\n",
    "###### <span style=\"color:red\">  My Comment (judge the bias and variance from plot visualization)</span>\n",
    "* trade off between bias and variance\n",
    "* way to judge and exam the data: train set error vs dev set error, for example:\n",
    "    * 1% vs. 11% -> high variance\n",
    "    * 15% vs. 16% -> high bias\n",
    "    * 15% vs. 30% -> high var and high bias\n",
    "    * 0.5% vs. 1% -> low var and low bias\n",
    "* Compare with \"human error / Bayes error / Optimal error\" to judge the value of obtained error\n",
    "\n",
    "### <ins> * Video · Basic Recipe for Machine Learning*</ins>\n",
    "###### <span style=\"color:red\">  My Comment </span>\n",
    "* algorithm has high bias for training set?\n",
    "    * Yes -> try bigger network and/or training longer -> until reduce to acceptable the bias\n",
    "    * No -> High variance?\n",
    "* High variance (dev set) \n",
    "    * Yes -> More data or regularization(?)\n",
    "    * No -> Done\n",
    "* bias and var trade-off\n",
    "    * no tool of changing just one of them.\n",
    " \n",
    "### <ins> * Video · Regularization*</ins>\n",
    "###### <span style=\"color:red\">  My Comment </span>\n",
    "* to prevent over fitting: penalizing the weight for being large. \n",
    "* $\\lambda$: regularization paramenter, in Python, use ```\\lambd to prevent clash with system parameter \\lambda```\n",
    "* logistic regression\n",
    "    * $J(w, b) = 1/M \\ \\sum L + \\lambda/ 2M  \\ ||w||^2_2$\n",
    "    * no regularizaiton term for b\n",
    "    * $L_2$ regularization: $||w||^2_2 = \\sum_{j=1}^{n_x} w_j^2 =w^T * w$\n",
    "    * $L_1$ regularizaiton: $L_1$ norm\n",
    "* NN\n",
    "    * regulariztion term: $ \\lambda /2M \\  \\sum_l \\ ||w^{[l]}||^2_F$\n",
    "    * $||w^{[l]}||^2_F = \\sum_{i=1}^{n^{[l-1]}} \\ \\sum_{j=1}^{n^{[l]}} w^{[l]}_{ij} $  \\# dim is # of layer $l-1$ and $l$\n",
    "    * frebnius norm instead of $L_2$ norm as $L_2$ norm is called as \"weight decay\" compared with the one without regularization term.\n",
    "   \n",
    "![weight decay l_2 norm](./pics/l_2_norm.PNG)\n",
    "* * *\n",
    "\n",
    "### <ins> * Video · Why regularization reduces overfitting?*</ins>\n",
    "###### <span style=\"color:red\">  My Comment </span>\n",
    "Intuition:\n",
    "1. $w$ small -> hidden nodes are useless/ little impact -> deep NN turns to be simplified -> can not fit to the data and result in high bias -> we wish medium $w$\n",
    "* tanh activation function: if $\\lambda$ large -> by penalization from the regularization term, $w$ will be relatively small -> $z$ small -> g(z) linear -> becomes around linear NN -> can not implement complex functions -> aviod overfitting\n",
    "\n",
    "![weight decay l_2 norm](./pics/overfitting.PNG)\n",
    "* * *\n",
    "* remeber to use the cost function with regularization term when do gradient descent check (plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Dropout Regularization*</ins>\n",
    "<span style=\"color:red\">  My comment </span>\n",
    "* to aviod over fitting\n",
    "* by dropping/eliminating a percentage of nodes at selected layer \n",
    "* implementation, for example, in layer 3, keep.prob=0.8/dropout rate=0.2\n",
    "``` \n",
    "d3=np.random.rand(as.shape[0],as.shape[1])<keep.prob \n",
    "a3=np.multiply (as,d3)\n",
    "a3 /= keep.prob # to keep the same expectation of z_4 as withouting the effect of dropping nodes z_4 = w^T * a^{[3]} + b. \n",
    "```\n",
    "    * thus will make evaluation (test set) easy, no scale problem\n",
    "* random choosing which nodes to drop\n",
    "* For test set, no drop out \n",
    "    \n",
    "### <ins> * Video · Understanding Dropout*</ins>\n",
    "<span style=\"color:red\">  My comment </span>  \n",
    "Intuition:\n",
    "1. Can't rely on any one feature, so have to spread out weights -> shrink norm of weight \n",
    "2. large computation complexity for layers with high dimensional weigh\n",
    "    * can apply different dropout rate for different layer\n",
    "* turn off drop off to check the cost funtion plot to ensure the gradient descent works.\n",
    "\n",
    "\n",
    "### <ins> * Video · Other regularization methods*</ins>\n",
    "<span style=\"color:red\">  My comment </span>\n",
    "1. Data augmentation\n",
    "    * to add/gain additional free training data\n",
    "        * e.g., random rotation of figures, random room in/out \n",
    "2. Early stopping\n",
    "    * plot training set error/cost function and dev set error against \\#iteration at the same time\n",
    "    * $w \\approx 0$ $\\rightarrow$ gradually become medium febenius norm -> large. early stop will improve overfitting.\n",
    "    \n",
    "![early stopping](./pics/early_stopping.PNG)\n",
    "* * *\n",
    "\n",
    "3. Solution of finding balance between optimization cost function and avoiding over fitting:\n",
    "    1. choose $L_2$ norm and try different regularizaion parameter (preferred by author)\n",
    "    2. try different values of weight and find the balance point in the early stopping plot.\n",
    "\n",
    "\n",
    "### <ins> * Video · Normalizing inputs*</ins>\n",
    "Normalization will make the cost function more symmetric -> faster learning rate (more direct towards the minimum)\n",
    "\n",
    "### <ins> * Video · Vanishing / Exploding gradients*</ins>\n",
    "<span style=\"color:red\">  My comment </span>  \n",
    "for a very deep NN with linear activation function, small weight/ large weight will cause $\\hat{y}$ -> vanish (0) or explode (+-$\\infty$) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Weight Initialization for Deep Networks*</ins>\n",
    "<span style=\"color:red\">  My comment:</span>  \n",
    "If \\# node large -> initially want samll $w$ -> want var(w)=2/n for ReLu -> $w^{[l]}$ = ```np.random.rand(shape)*npsqrt```  $(2/ n^{[l-1]} )$ \n",
    "* tanh: $\\sqrt{(1/ n^{[l-1]} )}$\n",
    "* others: $\\sqrt{(1/ ( n^{[l-1]} * n^{[l]}) )}$\n",
    "\n",
    "### <ins> * Video · Numerical approximation of gradients*</ins>\n",
    "\n",
    "### <ins> * Video · Gradient checking*</ins>\n",
    "<span style=\"color:red\">  My comment:</span>  \n",
    "* concatenate parameters together to form $\\theta = (w^{[1]},b^{[1]},w^{[2]},b^{[2]}...) \\Rightarrow J(\\cdot) -> J (\\theta)$\n",
    "\n",
    "* concatenate parameters together to form $d \\ \\theta = (d \\ w^{[1]},d \\ b^{[1]},d \\ w^{[2]},d \\ b^{[2]}...) $\n",
    "\n",
    "Gradient checking:\n",
    "* check whether $d \\ \\theta_{apprx} \\approx d \\ \\theta$ ?\n",
    "\n",
    "![grad_check](pics/grad_check.png)\n",
    "\n",
    "* * *\n",
    "\n",
    "### <ins> * Video · Gradient Checking Implementation Notes*</ins>\n",
    "<span style=\"color:red\">  My comment what is bug?</span>  \n",
    "Critial tips when implementation:\n",
    "1. Don't use in training - only to debug\n",
    "2. If algorithm fails gradient check, look at components to try to indentify bug. \n",
    "3. Remember regularization. (when calculate dw)\n",
    "4. Does not work with dropout. -> keep drop out = 0\n",
    "    * turn off drop out -> make sure gradient descent correct -> turn on drop off\n",
    "* <span style=\"color:red\"> Run gradient check at random initialization and after some time of training. </span> \n",
    "\n",
    "### <ins> * Quiz · Practical aspects of deep learning*</ins>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Other · Initialization*</ins>\n",
    "### <ins> * Programming Assignment · Initialization*</ins>\n",
    "### <ins> * Other · Regularization*</ins>\n",
    "### <ins> * Programming Assignment · Regularization*</ins>\n",
    "### <ins> * Other · Gradient Checking*</ins>\n",
    "### <ins> * Programming Assignment · Gradient Checking*</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEEK 2: Optimization algorithms\n",
    "\n",
    "Aims to train the model faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Mini-batch gradient descent*</ins>\n",
    "###### <span style=\"color:red\">  My Comment </span>\n",
    "Motivation: \n",
    "1. vectorization allows efficiently compute M examples.\n",
    "2. if n is large, going through the whole training set and update the parameter is slow.  \n",
    "\n",
    "**Notation**:\n",
    "![minibatch](./pics/minibatch.PNG)\n",
    "* * *\n",
    "\n",
    "**Implementation**  \n",
    "Vectorization for each mini-batch.\n",
    "![minibatch_imp](./pics/minibatch_imp.PNG)\n",
    "* * *\n",
    "\n",
    "* Save computation time for a large training set\n",
    "* can update the parameter at any nimi-batch\n",
    "\n",
    "### <ins> * Video · Understanding mini-batch gradient descent*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>\n",
    "Batch Gradient descent vs. mini-batch gradient descent\n",
    "* J is decreasing in #iterations(whole training set) vs. $J^{\\{t\\}}$ is not absolutely decreasing in $\\{t\\}$, but has the decreasing trend.\n",
    "\n",
    "mini-batch size\n",
    "* =M, batch gradient descent (too long per iteration)\n",
    "* =1 stochastic gradient descent (every example is a mini-batch) (lose speed up from vetorization)\n",
    "* in practice, mini-batch is between 1 and M -> not too big or small -> fastest learning (1, vectoriztion; 2, without processing entire training set.)\n",
    "\n",
    "* if m<=2000 -> mini-batch size =1\n",
    "* typical value: 64, 128, 256, 512,... (make sure mini-batch size fits in the CPU/GPU 2^6, 2^7, 2^8, 2^9)\n",
    "* choose and try different powers and descide which one is most efficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Exponentially weighted averages*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "For temprature, $v_t =\\beta v_{t-1} +(1-\\beta) \\theta_t $ \n",
    "* $v_t$ is the Exponentially weighted averages at time t\n",
    "* $\\theta_t$ is the temprature at time t\n",
    "* approximately weighted average over $1/(1-\\beta)$ days of temprature $\\theta$\n",
    "    \n",
    "### <ins> * Video · Understanding exponentially weighted averages*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "The weights sum up to approximatedly 1 -> apply bias correction\n",
    "![exp_weight](pics/exp_weight.png)\n",
    "* * *\n",
    "\n",
    "**Implementation**   \n",
    "Not as accurate as average of 1/ (1-\\beta) days of value, but is computational efficient.\n",
    "\n",
    "![exp_weight_imp](pics/exp_weight_imp.png)\n",
    "* * *\n",
    "\n",
    "\n",
    "### <ins> * Video · Bias correction in exponentially weighted averages*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "Bias correction (especially when t is small (initial period)):\n",
    "$v_t \\ / \\ (1-\\beta^t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Gradient descent with momentum*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "The implementation is in use of exponentially weighted average, can speedup the gradient descent.\n",
    "\n",
    "** weighted average of dw as the dw in gradiend descent**\n",
    "\n",
    "![momentum](./pics/momentum.PNG)\n",
    "* * *\n",
    "Almost no need of using bias correction, as will be ok after a few iteration  \n",
    "\n",
    "commonly set $\\beta=0.9$\n",
    "![momentum](./pics/momentum_imp.PNG)\n",
    "* * *\n",
    "\n",
    "### <ins> * Video · RMSprop*</ins>\n",
    "used to speedup the gradient descent.\n",
    "\n",
    "** weighted average of (dw)^2 (element wise) as the dw in gradiend descent**\n",
    "\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "![RMS](./pics/RMS.PNG)\n",
    "* * *\n",
    "\n",
    "### <ins> * Video · Adam optimization algorithm*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "Combination of momentum and RMS.\n",
    "![adam](./pics/adam.PNG)\n",
    "* * *\n",
    "** Adam (adaptive moment estimation), usually use the common parameter for adam paramters **\n",
    "![adam_parameters](./pics/adam_para.PNG)\n",
    "* * *\n",
    "weighted average of first moment ($\\beta_1$) and second moment ($\\beta_2$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Learning rate decay*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "Definition: slowly reduce the learning rate over time.\n",
    "Intuition:\n",
    "![learning_rate_decay_intuition](./pics/rate_decay_int.PNG)\n",
    "* * *\n",
    "\n",
    "1 epoch = 1 pass through training data\n",
    "\n",
    "$\\alpha = 1 \\ / \\  (1 + decay \\ rate \\ * \\ epoch \\ num )$\n",
    "\n",
    "![learning_rate_decay](./pics/rate_decay.PNG)\n",
    "* * *\n",
    "![learning_rate_decay_2](./pics/rate_decay_2.PNG)\n",
    "* * *\n",
    "manual: adajust manually according to the rate during training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · The problem of local optima*</ins>\n",
    "\n",
    "### <ins> * Quiz · Optimization algorithms*</ins>\n",
    "\n",
    "\n",
    "### <ins> * Other · Optimization*</ins>\n",
    "### <ins> * Programming Assignment · Optimization*</ins>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEEK 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Tuning process*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "Importance: red -> orange -> purple\n",
    "![tune_para](./pics/tune_para.PNG)\n",
    "* * *\n",
    "![tune_para_2](./pics/tune_para_2.PNG)\n",
    "\n",
    "Randomization enable the algorithm to try more values for each parameter\n",
    "\n",
    "* * *\n",
    "![tune_para](./pics/tune_para_3.PNG)\n",
    "* * *\n",
    "\n",
    "### <ins> * Video · Using an appropriate scale to pick hyperparameters*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>   \n",
    "For some parameters like #nodes, #layer -> randomly sample from a comfortable sets of values\n",
    "\n",
    "For $\\alpha$, choose appropriate scale\n",
    "* r from [a,b]=[-4,0] -> $\\alpha = 10^r$ ($10^{0.0001},...,1$)\n",
    "\n",
    "For exponentially weighted averages\n",
    "* $\\beta$ = 0.9,...0.999 ($\\iff$ average of 10 ,...,1000 samples)\n",
    "* r from [a,b] = [-3,-1] -> $1-\\beta = 10^r$ -> $\\beta = 1- 10^r$ \n",
    "\n",
    "\n",
    "### <ins> * Video · Hyperparameters tuning in practice: Pandas vs. Caviar*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "* less computer capacity -> pandas -> one at one time, monitor and adajust along trainging\n",
    "* ow, do parallel computing -> try many sets of hyperparameter at the same time -> choose the one the best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Normalizing activations in a network*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>    \n",
    "![normalization of z](./pics/normal_z.PNG)\n",
    "* * *\n",
    "![normalization of z 2](./pics/normal_z_2.PNG)\n",
    "* * *\n",
    "\n",
    "### <ins> * Video · Fitting Batch Norm into a neural network*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "![normalization of z in implementation](./pics/normal_z_imp.PNG)\n",
    "* * *\n",
    "```\n",
    "tf.nn.batch-normalization\n",
    "```\n",
    "\n",
    "Mini-batch:\n",
    "![normalization of z in implementation for mini-batch](./pics/normal_z_imp_mini.PNG)\n",
    "* * *\n",
    "Summary:\n",
    "![normalization of z in implementation for mini-batch summary](./pics/normal_z_imp_mini_summary.PNG)\n",
    "* * *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Why does Batch Norm work?*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "**Intuition**\n",
    "![batch_norm_intuition](./pics/batch_norm_int.PNG)\n",
    "\n",
    "**Batch norm eliminate the change of values in the early layer and make the values in the later layer more stable, parameters in each layer are learning more independently (faster)**\n",
    "\n",
    "thus the learnt parameter can fit in more general examples\n",
    "* * *\n",
    "\n",
    "![batch_norm](./pics/batch_norm.PNG)\n",
    "* * *\n",
    "![batch_norm_2](./pics/batch_norm_2.PNG)\n",
    "\n",
    "* Do not use batch norm as regularization.\n",
    "* * *\n",
    "\n",
    "### <ins> * Video · Batch Norm at test time*</ins>\n",
    "##### <span style=\"color:red\">  My Comment </span>  \n",
    "![batch_norm_test](./pics/batch_norm_test.PNG)\n",
    "* * *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Softmax Regression*</ins>\n",
    "###### <span style=\"color:red\">  My Comment </span>\n",
    "$C$ = \\# classes (including \"others\")\n",
    "\n",
    "$\\hat{y}=(p(class|x))_{classes}$; has dim $C \\times 1$\n",
    "\n",
    "**softmax activation function at layer L (last layer)**\n",
    "* $t = \\exp \\{ z ^{[l]}\\}$\n",
    "* $a =...$\n",
    "\n",
    "![softmax](./pics/sfmax.PNG)\n",
    "* * *\n",
    "\n",
    "Softmax: the boundary among classes is linear.\n",
    "\n",
    "### <ins> * Video · Training a softmax classifier*</ins>\n",
    "###### <span style=\"color:red\">  My Comment </span>\n",
    "* softmax ([0.1 0.1 0.7 0.1]) _vs._ hardmax ( [0 0 1 0])\n",
    "* Softmax regression is a generalization of logistic regression.\n",
    "\n",
    "** Cost/Loss function **\n",
    "![softmax](./pics/sfmax_ls.PNG)\n",
    "* * *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Video · Deep learning frameworks*</ins>\n",
    "\n",
    "### <ins> * Video · TensorFlow*</ins>\n",
    "###### <span style=\"color:red\">  My Comment </span>\n",
    "![tf_example](./pics/tf_example.PNG)\n",
    "* * *\n",
    "![tf_example](./pics/tf_example_2.PNG)\n",
    "* * *\n",
    "![tf_example](./pics/tf_example_3.PNG)\n",
    "* * *\n",
    "![tf_example](./pics/tf_example_4.PNG)\n",
    "* * *\n",
    "![tf_example](./pics/tf_example_5.PNG)\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1]\n",
      " [-10]\n",
      " [ 25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = np.array([[1] ,[-10], [25]])\n",
    "print(coefficients)\n",
    "coefficients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "coefficients = np.array([[1.] ,[-10.], [25.]])\n",
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "# cost = w**2-10*w+25\n",
    "x = tf.placeholder(tf.float32,[3,1])\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "print(session.run(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "session.run(train,feed_dict={x:coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.99999\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session.run(train,feed_dict={x:coefficients} )\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins> * Quiz · Hyperparameter tuning, Batch Normalization, Programming Frameworks*</ins>\n",
    "### <ins> * Other · Tensorflow*</ins>\n",
    "### <ins> * Programming Assignment · Tensorflow*</ins>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": "2",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
