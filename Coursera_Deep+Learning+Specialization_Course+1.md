
[Coursera Link](https://www.coursera.org/specializations/deep-learning#courses)
# Course 1: Neural Networks and Deep Learning 

**About the Course**

If you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new "superpower" that will let you build AI systems that just weren't possible a few years ago. In this course, you will learn the foundations of deep learning. When you finish this class, you will: - Understand the major technology trends driving Deep Learning - Be able to build, train and apply fully connected deep neural networks - Know how to implement efficient (vectorized) neural networks - Understand the key parameters in a neural network's architecture This course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions. This is the first course of the Deep Learning Specialization.



<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#WEEK-1:-Introduction-to-deep-learning" data-toc-modified-id="WEEK-1:-Introduction-to-deep-learning-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>WEEK 1: Introduction to deep learning</a></span><ul class="toc-item"><li><span><a href="#-Video-·-Welcome" data-toc-modified-id="-Video-·-Welcome-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span><ins> <em>Video · Welcome</em></ins></a></span></li><li><span><a href="#--Video-·-What-is-a-neural-network?-" data-toc-modified-id="--Video-·-What-is-a-neural-network?--1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span><ins> <em> Video · What is a neural network? </em></ins></a></span></li><li><span><a href="#--Video-·-Supervised-Learning-with-Neural-Networks-" data-toc-modified-id="--Video-·-Supervised-Learning-with-Neural-Networks--1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span><ins> <em> Video · Supervised Learning with Neural Networks </em></ins></a></span></li><li><span><a href="#--Video-·-Why-is-Deep-Learning-taking-off?" data-toc-modified-id="--Video-·-Why-is-Deep-Learning-taking-off?-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span><ins> <em> Video · Why is Deep Learning taking off?</em></ins></a></span></li><li><span><a href="#--Video-·-About-this-Course-" data-toc-modified-id="--Video-·-About-this-Course--1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span><ins> <em> Video · About this Course </em></ins></a></span></li><li><span><a href="#--Reading-·-Frequently-Asked-Questions" data-toc-modified-id="--Reading-·-Frequently-Asked-Questions-1.6"><span class="toc-item-num">1.6&nbsp;&nbsp;</span><ins> <em> Reading · Frequently Asked Questions</em></ins></a></span></li><li><span><a href="#--Video-·-Course-Resources" data-toc-modified-id="--Video-·-Course-Resources-1.7"><span class="toc-item-num">1.7&nbsp;&nbsp;</span><ins> <em> Video · Course Resources</em></ins></a></span></li><li><span><a href="#--Reading-·-How-to-use-Discussion-Forums" data-toc-modified-id="--Reading-·-How-to-use-Discussion-Forums-1.8"><span class="toc-item-num">1.8&nbsp;&nbsp;</span><ins> <em> Reading · How to use Discussion Forums</em></ins></a></span></li><li><span><a href="#--Quiz-·-In-troduction-to-deep-learning" data-toc-modified-id="--Quiz-·-In-troduction-to-deep-learning-1.9"><span class="toc-item-num">1.9&nbsp;&nbsp;</span><ins> <em> Quiz · In troduction to deep learning</em></ins></a></span></li></ul></li><li><span><a href="#WEEK-2:-Neural-Networks-Basics" data-toc-modified-id="WEEK-2:-Neural-Networks-Basics-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>WEEK 2: Neural Networks Basics</a></span><ul class="toc-item"><li><span><a href="#--Video-·-Binary-Classification" data-toc-modified-id="--Video-·-Binary-Classification-2.1"><span class="toc-item-num">2.1&nbsp;&nbsp;</span><ins> <em> Video · Binary Classification</em></ins></a></span></li><li><span><a href="#--Video-·-Logistic-Regression" data-toc-modified-id="--Video-·-Logistic-Regression-2.2"><span class="toc-item-num">2.2&nbsp;&nbsp;</span><ins> <em> Video · Logistic Regression</em></ins></a></span></li><li><span><a href="#--Video-·-Logistic-Regression-Cost-Function" data-toc-modified-id="--Video-·-Logistic-Regression-Cost-Function-2.3"><span class="toc-item-num">2.3&nbsp;&nbsp;</span><ins> <em> Video · Logistic Regression Cost Function</em></ins></a></span></li><li><span><a href="#--Video-·-Gradient-Descent" data-toc-modified-id="--Video-·-Gradient-Descent-2.4"><span class="toc-item-num">2.4&nbsp;&nbsp;</span><ins> <em> Video · Gradient Descent</em></ins></a></span></li><li><span><a href="#--Video-·-Derivatives" data-toc-modified-id="--Video-·-Derivatives-2.5"><span class="toc-item-num">2.5&nbsp;&nbsp;</span><ins> <em> Video · Derivatives</em></ins></a></span></li><li><span><a href="#--Video-·-More-Derivative-Examples" data-toc-modified-id="--Video-·-More-Derivative-Examples-2.6"><span class="toc-item-num">2.6&nbsp;&nbsp;</span><ins> <em> Video · More Derivative Examples</em></ins></a></span></li><li><span><a href="#--Video-·-Computation-graph" data-toc-modified-id="--Video-·-Computation-graph-2.7"><span class="toc-item-num">2.7&nbsp;&nbsp;</span><ins> <em> Video · Computation graph</em></ins></a></span></li><li><span><a href="#--Video-·-Derivatives-with-a-Computation-Graph" data-toc-modified-id="--Video-·-Derivatives-with-a-Computation-Graph-2.8"><span class="toc-item-num">2.8&nbsp;&nbsp;</span><ins> <em> Video · Derivatives with a Computation Graph</em></ins></a></span></li><li><span><a href="#--Video-·-Logistic-Regression-Gradient-Descent" data-toc-modified-id="--Video-·-Logistic-Regression-Gradient-Descent-2.9"><span class="toc-item-num">2.9&nbsp;&nbsp;</span><ins> <em> Video · Logistic Regression Gradient Descent</em></ins></a></span></li><li><span><a href="#--Video-·-Gradient-Descent-on-m-Examples" data-toc-modified-id="--Video-·-Gradient-Descent-on-m-Examples-2.10"><span class="toc-item-num">2.10&nbsp;&nbsp;</span><ins> <em> Video · Gradient Descent on m Examples</em></ins></a></span></li><li><span><a href="#--Video-·-Vectorization" data-toc-modified-id="--Video-·-Vectorization-2.11"><span class="toc-item-num">2.11&nbsp;&nbsp;</span><ins> <em> Video · Vectorization</em></ins></a></span></li><li><span><a href="#--Video-·-More-Vectorization-Examples" data-toc-modified-id="--Video-·-More-Vectorization-Examples-2.12"><span class="toc-item-num">2.12&nbsp;&nbsp;</span><ins> <em> Video · More Vectorization Examples</em></ins></a></span></li><li><span><a href="#--Video-·-Vectorizing-Logistic-Regression" data-toc-modified-id="--Video-·-Vectorizing-Logistic-Regression-2.13"><span class="toc-item-num">2.13&nbsp;&nbsp;</span><ins> <em> Video · Vectorizing Logistic Regression</em></ins></a></span></li><li><span><a href="#--Video-·-Vectorizing-Logistic-Regression's-Gradient-Output" data-toc-modified-id="--Video-·-Vectorizing-Logistic-Regression's-Gradient-Output-2.14"><span class="toc-item-num">2.14&nbsp;&nbsp;</span><ins> <em> Video · Vectorizing Logistic Regression's Gradient Output</em></ins></a></span></li><li><span><a href="#--Video-·-Broadcasting-in-Python" data-toc-modified-id="--Video-·-Broadcasting-in-Python-2.15"><span class="toc-item-num">2.15&nbsp;&nbsp;</span><ins> <em> Video · Broadcasting in Python</em></ins></a></span></li><li><span><a href="#--Video-·-A-note-on-python/numpy-vectors" data-toc-modified-id="--Video-·-A-note-on-python/numpy-vectors-2.16"><span class="toc-item-num">2.16&nbsp;&nbsp;</span><ins> <em> Video · A note on python/numpy vectors</em></ins></a></span></li><li><span><a href="#--Video-·-Quick-tour-of-Jupyter/iPython-Notebooks" data-toc-modified-id="--Video-·-Quick-tour-of-Jupyter/iPython-Notebooks-2.17"><span class="toc-item-num">2.17&nbsp;&nbsp;</span><ins> <em> Video · Quick tour of Jupyter/iPython Notebooks</em></ins></a></span></li><li><span><a href="#--Video-·-Explanation-of-logistic-regression-cost-function-(optional)" data-toc-modified-id="--Video-·-Explanation-of-logistic-regression-cost-function-(optional)-2.18"><span class="toc-item-num">2.18&nbsp;&nbsp;</span><ins> <em> Video · Explanation of logistic regression cost function (optional)</em></ins></a></span></li><li><span><a href="#--Quiz-·-Neural-Network-Basics" data-toc-modified-id="--Quiz-·-Neural-Network-Basics-2.19"><span class="toc-item-num">2.19&nbsp;&nbsp;</span><ins> <em> Quiz · Neural Network Basics</em></ins></a></span></li><li><span><a href="#--Reading-·-Deep-Learning-Honor-Code" data-toc-modified-id="--Reading-·-Deep-Learning-Honor-Code-2.20"><span class="toc-item-num">2.20&nbsp;&nbsp;</span><ins> <em> Reading · Deep Learning Honor Code</em></ins></a></span></li><li><span><a href="#--Reading-·-Programming-Assignment-FAQ" data-toc-modified-id="--Reading-·-Programming-Assignment-FAQ-2.21"><span class="toc-item-num">2.21&nbsp;&nbsp;</span><ins> <em> Reading · Programming Assignment FAQ</em></ins></a></span></li><li><span><a href="#--Other-·-Python-Basics-with-numpy-(optional)" data-toc-modified-id="--Other-·-Python-Basics-with-numpy-(optional)-2.22"><span class="toc-item-num">2.22&nbsp;&nbsp;</span><ins> <em> Other · Python Basics with numpy (optional)</em></ins></a></span></li><li><span><a href="#--Practice-Programming-Assignment-·-Python-Basics-with-numpy-(optional)" data-toc-modified-id="--Practice-Programming-Assignment-·-Python-Basics-with-numpy-(optional)-2.23"><span class="toc-item-num">2.23&nbsp;&nbsp;</span><ins> <em> Practice Programming Assignment · Python Basics with numpy (optional)</em></ins></a></span></li><li><span><a href="#--Other-·-Logistic-Regression-with-a-Neural-Network-mindset" data-toc-modified-id="--Other-·-Logistic-Regression-with-a-Neural-Network-mindset-2.24"><span class="toc-item-num">2.24&nbsp;&nbsp;</span><ins> <em> Other · Logistic Regression with a Neural Network mindset</em></ins></a></span></li><li><span><a href="#--Programming-Assignment-·-Logistic-Regression-with-a-Neural-Network-mindset" data-toc-modified-id="--Programming-Assignment-·-Logistic-Regression-with-a-Neural-Network-mindset-2.25"><span class="toc-item-num">2.25&nbsp;&nbsp;</span><ins> <em> Programming Assignment · Logistic Regression with a Neural Network mindset</em></ins></a></span></li></ul></li><li><span><a href="#WEEK-3:-Shallow-neural-networks" data-toc-modified-id="WEEK-3:-Shallow-neural-networks-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>WEEK 3: Shallow neural networks</a></span><ul class="toc-item"><li><span><a href="#--Video-·-Neural-Networks-Overview" data-toc-modified-id="--Video-·-Neural-Networks-Overview-3.1"><span class="toc-item-num">3.1&nbsp;&nbsp;</span><ins> <em> Video · Neural Networks Overview</em></ins></a></span></li><li><span><a href="#--Video-·-Neural-Network-Representation" data-toc-modified-id="--Video-·-Neural-Network-Representation-3.2"><span class="toc-item-num">3.2&nbsp;&nbsp;</span><ins> <em> Video · Neural Network Representation</em></ins></a></span></li><li><span><a href="#--Video-·-Computing-a-Neural-Network's-Output" data-toc-modified-id="--Video-·-Computing-a-Neural-Network's-Output-3.3"><span class="toc-item-num">3.3&nbsp;&nbsp;</span><ins> <em> Video · Computing a Neural Network's Output</em></ins></a></span></li><li><span><a href="#--Video-·-Vectorizing-across-multiple-examples" data-toc-modified-id="--Video-·-Vectorizing-across-multiple-examples-3.4"><span class="toc-item-num">3.4&nbsp;&nbsp;</span><ins> <em> Video · Vectorizing across multiple examples</em></ins></a></span></li><li><span><a href="#--Video-·-Explanation-for-Vectorized-Implementation" data-toc-modified-id="--Video-·-Explanation-for-Vectorized-Implementation-3.5"><span class="toc-item-num">3.5&nbsp;&nbsp;</span><ins> <em> Video · Explanation for Vectorized Implementation</em></ins></a></span></li><li><span><a href="#--Video-·-Activation-functions" data-toc-modified-id="--Video-·-Activation-functions-3.6"><span class="toc-item-num">3.6&nbsp;&nbsp;</span><ins> <em> Video · Activation functions</em></ins></a></span></li><li><span><a href="#--Video-·-Why-do-you-need-non-linear-activation-functions?" data-toc-modified-id="--Video-·-Why-do-you-need-non-linear-activation-functions?-3.7"><span class="toc-item-num">3.7&nbsp;&nbsp;</span><ins> <em> Video · Why do you need non-linear activation functions?</em></ins></a></span></li><li><span><a href="#--Video-·-Derivatives-of-activation-functions" data-toc-modified-id="--Video-·-Derivatives-of-activation-functions-3.8"><span class="toc-item-num">3.8&nbsp;&nbsp;</span><ins> <em> Video · Derivatives of activation functions</em></ins></a></span></li><li><span><a href="#--Video-·-Gradient-descent-for-Neural-Networks" data-toc-modified-id="--Video-·-Gradient-descent-for-Neural-Networks-3.9"><span class="toc-item-num">3.9&nbsp;&nbsp;</span><ins> <em> Video · Gradient descent for Neural Networks</em></ins></a></span></li><li><span><a href="#--Video-·-Backpropagation-intuition-(optional)" data-toc-modified-id="--Video-·-Backpropagation-intuition-(optional)-3.10"><span class="toc-item-num">3.10&nbsp;&nbsp;</span><ins> <em> Video · Backpropagation intuition (optional)</em></ins></a></span></li><li><span><a href="#--Video-·-Random-Initialization" data-toc-modified-id="--Video-·-Random-Initialization-3.11"><span class="toc-item-num">3.11&nbsp;&nbsp;</span><ins> <em> Video · Random Initialization</em></ins></a></span></li><li><span><a href="#--Quiz-·-Shallow-Neural-Networks" data-toc-modified-id="--Quiz-·-Shallow-Neural-Networks-3.12"><span class="toc-item-num">3.12&nbsp;&nbsp;</span><ins> <em> Quiz · Shallow Neural Networks</em></ins></a></span></li><li><span><a href="#--Other-·-Planar-data-classification-with-a-hidden-layer" data-toc-modified-id="--Other-·-Planar-data-classification-with-a-hidden-layer-3.13"><span class="toc-item-num">3.13&nbsp;&nbsp;</span><ins> <em> Other · Planar data classification with a hidden layer</em></ins></a></span></li><li><span><a href="#--Programming-Assignment-·-Planar-data-classification-with-a-hidden-layer" data-toc-modified-id="--Programming-Assignment-·-Planar-data-classification-with-a-hidden-layer-3.14"><span class="toc-item-num">3.14&nbsp;&nbsp;</span><ins> <em> Programming Assignment · Planar data classification with a hidden layer</em></ins></a></span></li></ul></li><li><span><a href="#WEEK-4:-Deep-Neural-Networks" data-toc-modified-id="WEEK-4:-Deep-Neural-Networks-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>WEEK 4: Deep Neural Networks</a></span><ul class="toc-item"><li><span><a href="#--Video-·-Deep-L-layer-neural-network" data-toc-modified-id="--Video-·-Deep-L-layer-neural-network-4.1"><span class="toc-item-num">4.1&nbsp;&nbsp;</span><ins> <em> Video · Deep L-layer neural network</em></ins></a></span></li><li><span><a href="#--Video-·-Forward-Propagation-in-a-Deep-Network" data-toc-modified-id="--Video-·-Forward-Propagation-in-a-Deep-Network-4.2"><span class="toc-item-num">4.2&nbsp;&nbsp;</span><ins> <em> Video · Forward Propagation in a Deep Network</em></ins></a></span></li><li><span><a href="#--Video-·-Getting-your-matrix-dimensions-right" data-toc-modified-id="--Video-·-Getting-your-matrix-dimensions-right-4.3"><span class="toc-item-num">4.3&nbsp;&nbsp;</span><ins> <em> Video · Getting your matrix dimensions right</em></ins></a></span></li><li><span><a href="#--Video-·-Why-deep-representations?" data-toc-modified-id="--Video-·-Why-deep-representations?-4.4"><span class="toc-item-num">4.4&nbsp;&nbsp;</span><ins> <em> Video · Why deep representations?</em></ins></a></span></li><li><span><a href="#--Video-·-Building-blocks-of-deep-neural-networks" data-toc-modified-id="--Video-·-Building-blocks-of-deep-neural-networks-4.5"><span class="toc-item-num">4.5&nbsp;&nbsp;</span><ins> <em> Video · Building blocks of deep neural networks</em></ins></a></span></li><li><span><a href="#--Video-·-Forward-and-Back-Propagation" data-toc-modified-id="--Video-·-Forward-and-Back-Propagation-4.6"><span class="toc-item-num">4.6&nbsp;&nbsp;</span><ins> <em> Video · Forward and Back Propagation</em></ins></a></span></li><li><span><a href="#--Video-·-Parameters-vs-Hyperparameters" data-toc-modified-id="--Video-·-Parameters-vs-Hyperparameters-4.7"><span class="toc-item-num">4.7&nbsp;&nbsp;</span><ins> <em> Video · Parameters vs Hyperparameters</em></ins></a></span></li><li><span><a href="#--Video-·-What-does-this-have-to-do-with-the-brain?" data-toc-modified-id="--Video-·-What-does-this-have-to-do-with-the-brain?-4.8"><span class="toc-item-num">4.8&nbsp;&nbsp;</span><ins> <em> Video · What does this have to do with the brain?</em></ins></a></span></li><li><span><a href="#--Quiz-·-Key-concepts-on-Deep-Neural-Networks" data-toc-modified-id="--Quiz-·-Key-concepts-on-Deep-Neural-Networks-4.9"><span class="toc-item-num">4.9&nbsp;&nbsp;</span><ins> <em> Quiz · Key concepts on Deep Neural Networks</em></ins></a></span></li><li><span><a href="#--Other-·-Building-your-Deep-Neural-Network:-Step-by-Step" data-toc-modified-id="--Other-·-Building-your-Deep-Neural-Network:-Step-by-Step-4.10"><span class="toc-item-num">4.10&nbsp;&nbsp;</span><ins> <em> Other · Building your Deep Neural Network: Step by Step</em></ins></a></span></li><li><span><a href="#--Programming-Assignment-·-Building-your-deep-neural-network:-Step-by-Step" data-toc-modified-id="--Programming-Assignment-·-Building-your-deep-neural-network:-Step-by-Step-4.11"><span class="toc-item-num">4.11&nbsp;&nbsp;</span><ins> <em> Programming Assignment · Building your deep neural network: Step by Step</em></ins></a></span></li><li><span><a href="#--Other-·-Deep-Neural-Network---Application" data-toc-modified-id="--Other-·-Deep-Neural-Network---Application-4.12"><span class="toc-item-num">4.12&nbsp;&nbsp;</span><ins> <em> Other · Deep Neural Network - Application</em></ins></a></span></li><li><span><a href="#--Programming-Assignment-·-Deep-Neural-Network-Application" data-toc-modified-id="--Programming-Assignment-·-Deep-Neural-Network-Application-4.13"><span class="toc-item-num">4.13&nbsp;&nbsp;</span><ins> <em> Programming Assignment · Deep Neural Network Application</em></ins></a></span></li></ul></li></ul></div>

what ```python code``` looks like in code environment

```python
print "Hello World"
```

## WEEK 1: Introduction to deep learning
Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.

### <ins> *Video · Welcome*</ins>
###### <span style="color:red">  My Comment </span>
Problems in course 3:
* validation set and training set come from different distribution
* end-to-end 

### <ins> * Video · What is a neural network? *</ins>
###### <span style="color:red">  My Comment </span>
* neuros	 
* activation function, for example, ReLu
* layers: input/hidden/output layers

### <ins> * Video · Supervised Learning with Neural Networks *</ins>
###### <span style="color:red">  My Comment </span>
* certain and well defined input features, target output variable
* Types of NN
    * RNN: Time series (audio, English translation) 
    * CNN: radar pics, video
    * Standard NN
* Type of data
    * structured data
        * statistical variables like: size, #bedrooms
    * unstructured data
        * audio, image, text
            
### <ins> * Video · Why is Deep Learning taking off?*</ins>

###### <span style="color:red">  My Comment </span>
1. Data: scale of data increase (accumulate in years)
    * performance as the amount of (labeled) data (for large training data set)
        * large NN > medium NN > small NN > traditional learning algorithm (SVM, logarithm regularization, etc.)
2. Computation
3. Algorithms, for example, change of activation function makes the gradient descent works better in cases.
    * ReLu; Sigmoid 
* How it works (circle process) : Idea -> code -> experiment ->idea  (to accelerate the speed)

### <ins> * Video · About this Course *</ins>
### <ins> * Reading · Frequently Asked Questions*</ins>
### <ins> * Video · Course Resources*</ins>
### <ins> * Reading · How to use Discussion Forums*</ins>
### <ins> * Quiz · In troduction to deep learning*</ins>


## WEEK 2: Neural Networks Basics
Learn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models.

### <ins> * Video · Binary Classification*</ins>
###### <span style="color:red">  My Comment </span>
* pic -> output y = 1 (cat) / 0 (non cat)
* pic: color channels: red, green, blue (64X64 pixels)
* input feature: $x = (red, green, blue)^T$; dim: $n_x = 64*64*3 =12288 $ 
* Training examples
    * $X=(x_1, x_2,...,x_M)$; dim: $n_x \times M$
    * $Y=(y_1, y_2,...,y_M)$; dim: $1 \times M$
    
### <ins> * Video · Logistic Regression*</ins>
###### <span style="color:red">  My Comment </span>
* given $x \in R^{n_x}$, output $\hat{y} = P(y=1|x)$, 
* Parameters: $w \in R^{n_x}$;$ b \in R$
* output $\textstyle\hat{y} = \sigma ( w^T x+b) $
    * sigmoid activation function:
    * $\sigma (z) =1/(1+e^{-z}) $    
    
### <ins> * Video · Logistic Regression Cost Function*</ins>
###### <span style="color:red">  My Comment </span>
given ${(x^{(1)},y^{(1)}),...(x^{(M)},y^{(M)})}$, want $\hat{y}^{(i)}~= y^{(i)}$
- loss function for logistic regression:  
   $ L(\hat{y},y) = - (y \log \hat{y} + ( 1-y )  \log (1-\hat{y})) $   
 - $ y =1$, want $\log (\hat{y})$ large, want $\hat{y}$ large
 - $y =0$, ...
- cost function  
    $J(w, b) = 1/M \ \sum_{i=1}^M L(\hat{y}^{(i)},  y^{(i)})$

### <ins> * Video · Gradient Descent*</ins>
###### <span style="color:red">  My Comment </span>
* Want to find w and b to minimize J(w, b)
* global optimum vs local optimum
* $\hat{y} = w^T x+b$ (logic behind)
* repeat do iterations:  

    1. $ w := w - \alpha \ d J(w, b) /d w $
        * $ \alpha$ is the learning rate 
        * $d J(w, b) /d w$ is derivative of $J(w,b)$ w.r.t $w$ (slope of the function $J(w)$ in the $w$ direction)
    
    2. $b := b - \alpha \ d J(w, b) /d b $  
* derivative symbol: $d$ (uni variable)  vs.  $\partial$  (partial derivative symbol) if two or more variables. 

### <ins> * Video · Derivatives*</ins>

### <ins> * Video · More Derivative Examples*</ins>

### <ins> * Video · Computation graph*</ins>
###### <span style="color:red">  My Comment </span>
forward computation  
$J(a, b, c) = 3(a+b \ c)$, let $ u=b\ c$, $v=a+u$, $J=3 \ v$

### <ins> * Video · Derivatives with a Computation Graph*</ins>
###### <span style="color:red">  My Comment </span>
backward computation
* $d J \ / \ d v =3$; $d J \ / \ d a= d J \ / \ d v \times d v \ / \ d a = 3$ ( chain rule)
* convenient behavior in coding: use "$ d \ var$ " to represent the derivative " $ d \ findoutputvar \ / \ d  \ var $ ". For example, use "$d\ a$ " to represent "$d J \ / \ d a$ ". 
* remark: use the value of the variables at the current iteration when the derivative contains that variable. For example, $ d b = 3 c = 6 $ if $c=2$ in this iteration.

### <ins> * Video · Logistic Regression Gradient Descent*</ins>
###### <span style="color:red">  My Comment </span>
* $z= w^T \ x+b$
* $\hat{y} = a = \sigma (z)$
* $L (a, y) = -\{ y \log (a)+(1-y) \log (1-a)\}$
* if $n_x=2$: 
    * forward computation
        *  $x_1,\ x_2,\ w_1,\ w_2,\ b \implies z= w_1 \ x_1 + w_2 \ x_2 +b \implies a= \sigma (z) \implies L (a, y)$
    * backward computation
        * $d a  \implies d z \implies d w_1,\ d w_2,\ d b$
    * Gradient decent ( repeat many iterations) (over one training example, M=1)
        * $w_1 := w_1 - \alpha \ d w_1$
        * $w_2 := w_2 - \alpha \ d w_2$
        * $b := b - \alpha \ d b$

### <ins> * Video · Gradient Descent on m Examples*</ins>
###### <span style="color:red">  My Comment </span>
* backward computation
    * $J(w, b) = 1/M \ \sum_{i=1}^M \ L(\hat{y}^{(i)},  y^{(i)})$
* In each iteration
    * Theoretically : $d w_1 = 1/M \ \sum_{i=1}^M \ d w_1^{(i)}$ ; $d w_2 = 1/M \   \sum_{i=1}^M \ d w_2^{(i)}$;$ d b = 1/M \   \sum_{i=1}^M \ d b^{(i)}$
    * Implementation
        * for $i =1, ..., M$, calculate
            * $z^{(i)} = w^T \ x^{(i)}+b$
            * $a^{(i)}= \sigma (z^{(i)})$
            * $J \ +=  -(y^{(i)} \ \log(a^{(i)})+(1-y^{(i)}) \ \log(1-a^{(i)}))$
            * $d a^{(i)} $
            * $d z^{(i)} $
            * $d w_1 \ += d w_1^{(i)} $
            * $d w_2 \ += d w_2^{(i)}$
            * $d b \ += d b^{(i)}$
        * $ d w_1 \ /= M$
        * $d w_2 \ /= M$
        * $d b \ /= M$
    * $w_1 := w_1 - \alpha \ d w_1$
    * $w_2 := w_2 - \alpha \ d w_2$
    * $ b := b - \alpha \ d b$

### <ins> * Video · Vectorization*</ins>
###### <span style="color:red">  My Comment </span>
* Computation time (np.dot vs for loop)
* $J = np.dot(w,x)+b$ vs  $J(w, b) = 1/M \ \sum_{i=1}^M L(\hat{y}^{(i)},  y^{(i)})$
python code:


```python
import pandas as pd
import numpy as np
import time
dim=10**6
# np.dot
w = np.random.rand(dim)
x = np.random.rand(dim)
tic=time.time()
J = np.dot(w,x)
toc=time.time()
print ("J = %f"%J)
print("computation time of using np.dot is %f ms"%(toc-tic))

# for loop
tic=time.time()
J = 0
for i in range(dim):
    J += w[i]*x[i]
toc=time.time()
print ("J = %f"%J)
print("computation time of using for loop is %f ms"%(toc-tic))

```

    J = 249965.257150
    computation time of using np.dot is 0.047034 ms
    J = 249965.257150
    computation time of using for loop is 2.783976 ms
    


### <ins> * Video · More Vectorization Examples*</ins>
### <ins> * Video · Vectorizing Logistic Regression*</ins>
### <ins> * Video · Vectorizing Logistic Regression's Gradient Output*</ins>
### <ins> * Video · Broadcasting in Python*</ins>
### <ins> * Video · A note on python/numpy vectors*</ins>
### <ins> * Video · Quick tour of Jupyter/iPython Notebooks*</ins>
### <ins> * Video · Explanation of logistic regression cost function (optional)*</ins>
###### <span style="color:red">  My Comment </span>
pdf of logistic regression is:  
* 1 training sample has likelihood function:  
$ p(y | x) = \hat{y}^ y \ (1-\hat{y})^{(1-y)} \implies \log p(y | x) = y \log \hat{y} + {(1-y) \log (1-\hat{y})} $  
* M training samples has likelihood function: 

$\log \prod_{i=1}^M p(y^{(i)} | x^{(i)}) = \sum_{i=1}^M \{ y^{(i)} \log \hat{y}^{(i)} + {(1-y^{(i)}) \log (1-\hat{y}^{(i)})} \} $

$\implies MLE \iff minimizing \ M \times J(w,b)  \iff Minimizing \ J(w,b)$


### <ins> * Quiz · Neural Network Basics*</ins>
### <ins> * Reading · Deep Learning Honor Code*</ins>
### <ins> * Reading · Programming Assignment FAQ*</ins>
### <ins> * Other · Python Basics with numpy (optional)*</ins>
### <ins> * Practice Programming Assignment · Python Basics with numpy (optional)*</ins>
### <ins> * Other · Logistic Regression with a Neural Network mindset*</ins>
### <ins> * Programming Assignment · Logistic Regression with a Neural Network mindset*</ins>



## WEEK 3: Shallow neural networks
Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.

### <ins> * Video · Neural Networks Overview*</ins>
###### <span style="color:red">  My Comment </span>

### <ins> * Video · Neural Network Representation*</ins>
###### <span style="color:red">  My Comment </span>
Notation convention: 
* don't count the input layer, for example, input layer+hidden layer+output layer, is called a 2 layer NN.
* $a^{[l]}_i$ represents the the node $i$ in layer $l$ 
* $a^{[l]}_i = \sigma ((w_i^{[l]})^T \ x + b_i^{[l]})$
* at layer $l$, 
    * use $w^{[l]} = ((w_i^{[l]})_i$ to represent the weight matrix, which has dim $dim \ of \ layer \ l \times dim \ of \ feature \ vector $
    * use $b^{[l]}= (b_i^{[l]})_i$ to represent the constant vector, which has dim $dim \ of \ layer \ l \times 1 $
    
### <ins> * Video · Computing a Neural Network's Output*</ins>
### <ins> * Video · Vectorizing across multiple examples*</ins>
###### <span style="color:red">  My Comment </span>
for M training samples, vectorizing by stack them in columns:
* $X = (x^{(1)},...x^{(M)})$, where $x^{(i)}$ is the $ith$ training sample (feature vector)
    * $X$ has dim $dim \ of \ feature  \ vector \times M $
    
* $Z^{[l]} = (z^{[l](1)},...z^{[l](M)}) $ is the node vector at layer $l$  
    * $z^{[l](i)}=  w^{[l]} x^{(i)} +b^{[l]}$
    * M training samples share the same $w^{[l]}$ and $b^{[l]}$ for layer $l$


### <ins> * Video · Explanation for Vectorized Implementation*</ins>
### <ins> * Video · Activation functions*</ins>
###### <span style="color:red">  My Comment </span>
use $g(\cdot)$ to represnt the activation function
* tanh activation function: 
    * $tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$
    * belongs to (-1,1)
* ReLu: $max\{0,z\}$ 
* leaky ReLu: $ \{-0.01 z \ if \ z<0, z \ if \ z \ge 0 \}$
* usually use tanh/ reLu /(rectified) leaky reLu for hidden layers and sigmoid / reLu /(rectified) leaky reLu for output layer
* faster learning rate for reLu, 
* use sigmoid at output layer unless for value classification

### <ins> * Video · Why do you need non-linear activation functions?*</ins>
###### <span style="color:red">  My Comment </span>
* layers of linear activation function $\iff$ 1 layer of linear
* use non-linear activation function for hidden layers
* consider linear activation function for output layer

### <ins> * Video · Derivatives of activation functions*</ins>
###### <span style="color:red">  My Comment </span>
* sigmoid activation function: $g(z)= 1/(1+e^{-z})$
    * $g^{\prime} (z)=d \ g(z) /d \ z = g(z) (1-g(z))$
* tanh: $g(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$
* ReLu: $g(z)=$
    * $ g^{\prime} (z) =\{0 \ if \ z<0 ; \ 1 \ if \ z\ge 0\}$ 

### <ins> * Video · Gradient descent for Neural Networks*</ins>
###### <span style="color:red">  My Comment </span>
notations: $n^{[l]}$ the number of nodes at layer $l$. $n^{[0]} = n_x$, which is the dim of feature vector.
* at layer $l$, parameters are $w^{[l]}$ and $b^{[l]}$, $w^{[l]}$ has dim $n^{[l]}\times n^{[l-1]}$, $b^{[l]}$ has dim $n^{[l]}\times 1$.

Gradient descent, repeatly do the following iterations:
* forward propogation for $\hat{y}$
* back propogation for $ d \ parameters $
* update the parameters

![Example of consequence of large weight.](pics/gradient descent.png)
*********

### <ins> * Video · Backpropagation intuition (optional)*</ins>
###### <span style="color:red">  My Comment </span>


### <ins> * Video · Random Initialization*</ins>
###### <span style="color:red">  My Comment </span>
zero initializion of weights will result in symmetric nodes in hidden layers, thus are computing same function for nodes in the hidden layer, which is pointless. 

small weight in initialization: w= random(,)*0.01  
* resemble the problem in triaing procedure: large weights of nodes -> large z -> small or 0 $g^{\prime}$ -> slow learning rate or stucked learning procedure.

![Example of consequence of large weight.](pics/random_initialization.png)
*********

### <ins> * Quiz · Shallow Neural Networks*</ins>
### <ins> * Other · Planar data classification with a hidden layer*</ins>
### <ins> * Programming Assignment · Planar data classification with a hidden layer*</ins>



## WEEK 4: Deep Neural Networks
Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision.

### <ins> * Video · Deep L-layer neural network*</ins>
###### <span style="color:red">  My Comment </span>
* shallow NN: just th input and output layer, namely, 1 layer NN.
* notations: 
    * L - # layers in NN; 
    * $n^{[l]}$ - # nodes in layer $l$
    * $a^{[l]}$ - activation 
    * $w^{[l]}$ - weights for $z^{[l]}$
    * $b^{[l]}$ - bias for $z^{[l]}$

### <ins> * Video · Forward Propagation in a Deep Network*</ins>
###### <span style="color:red">  My Comment </span>
at layer $l$,
* $z^{[l]}=w^{[l]} \ a^{[l-1]} + b^{[l]}$
    * $z^{[l]}$ - dim: $n^{[l]}\times 1$
    * $w^{[l]}$ - dim $n^{[l]}\times n^{[l-1]}$
    * $b^{[l]}$ - dim $n^{[l]}\times 1$
* $a^{[l]}= g(z^{[l]}) $, dim: $n^{[l]}\times 1$  
* $a^{[0]} = x$

Vectorization for M training examples,
* $Z^{[l]}=w^{[l]} \ A^{[l-1]} + b^{[l]}$, 
    * $Z^{[l]}$ - dim: $n^{[l]}\times M$
    * $w^{[l]}$ - dim $n^{[l]}\times n^{[l-1]}$
    * $b^{[l]}$ - dim $n^{[l]}\times 1$
* $A^{[l]}= g(Z^{[l]}) $, dim: $n^{[l]}\times M$
* $A^{[0]} = X$

### <ins> * Video · Getting your matrix dimensions right*</ins>
###### <span style="color:red">  My Comment </span>
from simple to complex,for example,
* face recognition: edges -> parts -> face
* audio: low -> ..->words--> sentence/phrase

### <ins> * Video · Why deep representations?*</ins>
###### <span style="color:red">  My Comment </span>
deep NN vs. swallow NN
* computation complexity: $O(\log(n))$ vs. $2^n$

### <ins> * Video · Building blocks of deep neural networks*</ins>

### <ins> * Video · Forward and Back Propagation*</ins>
###### <span style="color:red">  My Comment </span>
iteratively repeat the following:
* in layer $l$, parameters are $w^{[l]}$ and $b^{[l]}$
* forward, for $l$ in {0,...,L}
    * input $a^{[l-1]}$ --->  $w^{[l]}$ and $b^{[l]}$ with $z^{[l]}$ cached ---> output $a^{[l]}$
* backward, for $l$ in {L,...,0}
    * input $d \ a^{[l]}$ --->  $w^{[l]}$ and $b^{[l]}$ and $ d \ z^{[l]}$ ---> output $d \ a^{[l-1]}$  and $d \ w^{[l]}$ and $d \ b^{[l]}$ to update $w^{[l]}$ and $b^{[l]}$
    
Similarly to M training examples, except
* $d \ w^{[l]}$ and $d \ b^{[l]}$ is the average of $d \ w^{[l]}$ and $d \ b^{[l]}$ calculated using each of M examples.

![forward_back_propogation.](pics/forward_back.png)
*********

### <ins> * Video · Parameters vs Hyperparameters*</ins>
###### <span style="color:red">  My Comment </span>
Parameters:  $w^{[l]}$ and $b^{[l]}$ for $l $ in {0,...,L}  
Hyperparameters: 
* learning rate $\alpha$
* \# iterations
* \# hidden layers
* \# nodes in each hidden layers
* choice of activation functions

Later: mini-batches size,...

Empirical process:  
    Try different combinations/values of hyperparemeters and complete the circle idea -> code -> experiment -> code.

### <ins> * Video · What does this have to do with the brain?*</ins>
###### <span style="color:red">  My Comment </span>


### <ins> * Quiz · Key concepts on Deep Neural Networks*</ins>
### <ins> * Other · Building your Deep Neural Network: Step by Step*</ins>
### <ins> * Programming Assignment · Building your deep neural network: Step by Step*</ins>
### <ins> * Other · Deep Neural Network - Application*</ins>
### <ins> * Programming Assignment · Deep Neural Network Application*</ins>


